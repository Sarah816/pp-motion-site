<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="PAPER_TITLE - AUTHOR_NAMES">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>PAPER_TITLE - AUTHOR_NAMES | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- Title -->
            <h1 class="title is-1 publication-title" style="margin-bottom: 1rem;">
              PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation
            </h1>

            <!-- Authors -->
            <div class="authors" style="font-size: 1.1rem; line-height: 2rem;">
              <span class="author-block"><a href="#" target="_blank">Sihan Zhao</a><sup>*1</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Zixuan Wang</a><sup>*1</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Tianyu Luan</a><sup>†2</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Jia Jia</a><sup>†1</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Wentao Zhu</a><sup>3</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Jiebo Luo</a><sup>4</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Junsong Yuan</a><sup>2</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Nan Xi</a><sup>2</sup></span>
            </div>

            <!-- Affiliations -->
            <div class="affiliations" style="font-size: 1rem; color: #666; margin-top: 0.5rem; line-height: 1.6rem;">
              <sup>1</sup> Tsinghua University, Beijing, China<br>
              <sup>2</sup> University at Buffalo, Buffalo, NY, USA<br>
              <sup>3</sup> Eastern Institute of Technology, Ningbo, China<br>
              <sup>4</sup> University of Rochester, Rochester, NY, USA
              <span class="eql-cntrb"><br><sup>*</sup>Equal Contribution &nbsp;&nbsp; <sup>†</sup>Co-corresponding</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2508.08179.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- TODO: Replace with your GitHub repository URL -->
              <span class="link-block">
                <a href="https://github.com/YOUR REPO HERE" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
              </span>

              <span class="link-block">
                <a href="http://arxiv.org/abs/2508.08179" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <source src="static/videos/mfp1224-promp-video.mp4" type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <b>PP-Motion</b>, a data-driven metric for human motion fidelity that bridges the gap between
            <i>physical feasibility</i> and <i>human perception</i>. We generate fine-grained, continuous physical annotations by
            computing the minimum adjustments required to make a motion comply with physics simulation, and jointly train
            with human perceptual preferences via a correlation-based loss (PLCC) and a perceptual ranking loss. PP-Motion
            achieves stronger physical alignment and comparable or better perceptual alignment than prior metrics on
            MotionPercept subsets (MDM/FLAME).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            PP-Motion takes a motion sequence as input, encodes it into spatio-temporal features, and decodes them into a fidelity score. The training is supervised by two complementary signals. On one hand, we use human perceptual labels and apply a ranking loss that ensures human-preferred motions receive higher scores. On the other hand, we use fine-grained physical annotations and apply a correlation loss that aligns predictions with continuous physics-based labels. Specifically, we adopt Pearson correlation loss, which focuses on relative consistency and captures physical priors more effectively than simple regression. Together, these objectives enable the model to capture both perceptual realism and physical feasibility.
          </p>

          <p style="text-align: center;">
            <img src="static/images/pipeline.jpg" alt="Results" style="max-width: 100%; height: auto;">
            <br>
            <span style="font-size: 1.0rem; color: #555;">Figure: Overview of the PP-Motion training framework combining perceptual and physical supervision.</span>
          </p>

          <!-- <figure class="image" style="width: 100%; text-align: center;">
            <img src="static/images/pipeline.jpg" 
                 alt="Overview of PP-Motion framework" 
                 style="max-width: 100%; height: auto; border-radius: 10px;">
            <figcaption style="margin-top: 8px;">
              Figure: Overview of the PP-Motion training framework combining perceptual and physical supervision.
            </figcaption>
          </figure> -->

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset Construction</h2>
        <div class="content has-text-justified">
          <p>
            We build upon the MotionPercept dataset, which provides human perceptual labels. To add physical supervision, we generate new annotations with a physics simulator. For each motion, we train a correction network with reinforcement learning that makes only minimal adjustments until the motion conforms to physical laws. In practice, we consider a motion to follow physical laws if it can be successfully simulated in the physics engine.
          </p>
          <figure>
            <div class="columns is-centered">
              <div class="column is-half">
                <div class="image" style="text-align:center;">
                  <video controls style="max-width:100%; height:auto;">
                    <source src="static/videos/dataset1_simulate_raw_motion.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
              <div class="column is-half">
                <div class="image" style="text-align:center;">
                  <video controls style="max-width:100%; height:auto;">
                    <source src="static/videos/dataset1_simulate_imitated_motion.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </div>
            <figcaption class="has-text-centered mt-2">
              The original motion (left) contains physically implausible elements, which are adjusted in simulated motion (right). 
            </figcaption>
          </figure>

          <p>
            As shown in the video, when the original motion contains physically implausible elements, the simulator corrects them so that the sequence can be executed realistically. Conversely, if a motion is already physically valid, the corrected version closely matches the original. This demonstrates that our annotations provide fine-grained labels which capture physical feasibility more precisely and offer strong supervision for training PP-Motion.
          </p> 

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <p>
            PP-Motion shows clear improvements over previous metrics. Evaluated with three correlation measures—Pearson Linear Correlation Coefficient, Spearman Rank Order Correlation Coefficient, and Kendall Rank Order Correlation Coefficient — it achieves much higher correlation with physical annotations, demonstrating stronger alignment with physics. At the same time, it slightly improves perceptual accuracy compared to human-labeled baselines.
          </p>

          <p style="text-align: center;">
            <img src="static/images/result_table.png" alt="Results" style="max-width: 100%; height: auto;">
            <br>
            <span style="font-size: 1.0rem; color: #555;">Figure: Comparison with previous metrics.</span>
          </p>

          <p>
            To further illustrate how our metric works, consider the comparison between two videos. To human eyes, the left motion looks more reasonable. However, when tested in the physics simulator, the left motion shows clear floating problems, whereas the right motion is more physically plausible. Our metric successfully captures these differences, assigning scores that reflect both perception and physics.
          </p>

          <figure>
            <div class="columns is-centered">
              <div class="column is-half">
                <div class="image" style="text-align:center;">
                  <video controls style="max-width:100%; height:auto;">
                    <source src="static/videos/simulate_a1_better.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <figcaption class="has-text-centered mt-2">
                    Left: a motion judged “better” by human demonstrates physical issues
                    PP-Motion score: -0.23
                  </figcaption>
                </div>
              </div>
              <div class="column is-half">
                <div class="image" style="text-align:center;">
                  <video controls style="max-width:100%; height:auto;">
                    <source src="static/videos/simulate_a1_worse.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <figcaption class="has-text-centered mt-2">
                    Right: a motion judged “worse” by human shows greater physical plausibility
                    PP-Motion score: 0.77
                  </figcaption>
                </div>
              </div>
            </div>
          </figure>

        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{zhao2025ppmotionphysicalperceptualfidelityevaluation,
        title={PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation}, 
        author={Sihan Zhao and Zixuan Wang and Tianyu Luan and Jia Jia and Wentao Zhu and Jiebo Luo and Junsong Yuan and Nan Xi},
        year={2025},
        eprint={2508.08179},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2508.08179}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
